{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c00f1d0c-d493-4492-b6f0-c5c8819a8ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "import numpy as np\n",
    "from random import random, uniform, randrange, triangular\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59956032-d301-47ff-bc2e-906d687b03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from onnx import optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "058162c5-0f50-44f1-a5bc-329590ae3598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "tensor([[0.7225, 0.7538, 0.8294, 0.8637, 0.8681, 0.8234, 0.8683, 0.8890, 0.9146,\n",
      "         0.8543]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "# base_size = prev_data//group_base\n",
    "# IMU_size = prev_data//group_IMU\n",
    "# torque_size = (prev_data + fwd_data)//group_base\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(200, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "dummy = torch.zeros((1, 200,))\n",
    "input_var = Variable(dummy)\n",
    "model = torch.load('model_L1Norm.pth')\n",
    "print(model)\n",
    "m_inp = torch.ones((1, 200))\n",
    "print(model(m_inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11251a58-e588-43ca-83ab-bf1060feef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch2keras import pytorch_to_keras\n",
    "# we should specify shape of the input tensor\n",
    "k_model = pytorch_to_keras(model, input_var, verbose=False)  \n",
    "k_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4188fff7-12b5-41b2-8d0d-c1772580b581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_0 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "7 (Dense)                    (None, 64)                12864     \n",
      "_________________________________________________________________\n",
      "8 (Activation)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "9 (Dense)                    (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "10 (Activation)              (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "11 (Dense)                   (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "output_0 (Activation)        (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 17,674\n",
      "Trainable params: 17,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "k_model.save('model_s_keras.h5')\n",
    "# print(k_model.shape)\n",
    "reconstructed_model = keras.models.load_model(\"model_s_keras.h5\")\n",
    "print(reconstructed_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b21842-ffab-4bac-b518-100874340e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "Successfully got model architecture! üòÑ\n",
      "\u001b[0m\u001b[37m\n",
      "\u001b[38;5;220mLayers:\u001b[0m\u001b[37m\n",
      "  \u001b[38;5;74mname: InputLayer (ignored)\u001b[0m\u001b[37m\u001b[38;5;119m\n",
      " ===========================\n",
      "\u001b[0m\u001b[37m  \u001b[38;5;74mname: dense\u001b[0m\u001b[37m\n",
      "  \u001b[38;5;205mactivation: relu\u001b[0m\u001b[37m\n",
      "  \u001b[38;5;135mshape: (200, 64)\u001b[0m\u001b[37m\u001b[38;5;119m\n",
      " ===========================\n",
      "\u001b[0m\u001b[37m  \u001b[38;5;74mname: Activation (ignored)\u001b[0m\u001b[37m\u001b[38;5;119m\n",
      " ===========================\n",
      "\u001b[0m\u001b[37m  \u001b[38;5;74mname: dense\u001b[0m\u001b[37m\n",
      "  \u001b[38;5;205mactivation: relu\u001b[0m\u001b[37m\n",
      "  \u001b[38;5;135mshape: (64, 64)\u001b[0m\u001b[37m\u001b[38;5;119m\n",
      " ===========================\n",
      "\u001b[0m\u001b[37m  \u001b[38;5;74mname: Activation (ignored)\u001b[0m\u001b[37m\u001b[38;5;119m\n",
      " ===========================\n",
      "\u001b[0m\u001b[37m  \u001b[38;5;74mname: dense\u001b[0m\u001b[37m\n",
      "  \u001b[38;5;205mactivation: tanh\u001b[0m\u001b[37m\n",
      "  \u001b[38;5;135mshape: (64, 10)\u001b[0m\u001b[37m\u001b[38;5;119m\n",
      " ===========================\n",
      "\u001b[0m\u001b[37m  \u001b[38;5;74mname: Activation (ignored)\u001b[0m\u001b[37m\n",
      "\u001b[0m\u001b[37m\u001b[38;5;220m\n",
      "üî® Now building pure Python + NumPy model...\u001b[0m\u001b[37m\n",
      "\u001b[92müôå Saved Konverted model!\u001b[0m\u001b[37m\n",
      "\u001b[92mOutput model file: export/model_L1.py\u001b[0m\u001b[37m\n",
      "\u001b[92mWeights and biases file: export/model_L1_weights.npz\u001b[0m\u001b[37m\n",
      "\u001b[38;5;220m\n",
      "‚ùó Make sure to change the path inside the wrapper file to your weights if you move the file elsewhere.\u001b[0m\u001b[37m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<konverter.Konverter at 0x7f4d44631370>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import konverter\n",
    "konverter.konvert('model_s_keras.h5', output_file='export/model_L1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3560e7-884a-45ab-8dad-ba488f5d5c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
